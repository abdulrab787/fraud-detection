# ğŸ’³ Credit Card Fraud Detection â€” End-to-End ML Pipeline (Industry-Grade)

ğŸ” **Problem:**  
Detect fraudulent credit card transactions in a highly imbalanced dataset where fraudulent cases are rare but extremely costly.

ğŸ“Œ **Goal:**  
Build a **reproducible, production-style ML system** with:
- Robust preprocessing
- Strong feature engineering  
- Multiple models (baseline â†’ advanced)  
- Stacking ensemble  
- Threshold tuning (business-ready)  
- Explainability (SHAP)  
- Experiment tracking (MLflow)  
- Automated inference & submission pipeline  

---

## ğŸ† Performance Summary

| Model | Validation Performance |
|------|------------------------|
| Logistic Regression | Baseline |
| Random Forest | Lower than baseline |
| Tuned XGBoost | **Best single model** âœ… |
| Tuned LightGBM | Competitive |
| Stacking Ensemble | **Best overall (after threshold tuning)** ğŸš€ |

*(Exact numbers can be updated from your logs if you want.)*


## ğŸ“‚ Project Structure
credit-card-fraud/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â”œâ”€â”€ fraudTrain.csv
â”‚ â”‚ â””â”€â”€ fraudTest.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda.ipynb
â”‚ â”œâ”€â”€ 02_preprocessing.ipynb
â”‚ â”œâ”€â”€ 03_baseline_models.ipynb
â”‚ â”œâ”€â”€ 04_xgboost_tuning.ipynb
â”‚ â”œâ”€â”€ 05_lightgbm_tuning.ipynb
â”‚ â”œâ”€â”€ 06_stacking_ensemble.ipynb
â”‚ â”œâ”€â”€ 09_threshold_tuning.ipynb
â”‚ â””â”€â”€ 10_shap_explainability.ipynb
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ preprocessing.py
â”‚ â”œâ”€â”€ train_with_mlflow.py
â”‚ â””â”€â”€ make_submission.py
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ preprocessor.pkl
â”‚ â”œâ”€â”€ xgb_fraud_model_tuned.pkl
â”‚ â”œâ”€â”€ lgb_fraud_model_tuned.pkl
â”‚ â”œâ”€â”€ logreg_baseline.pkl
â”‚ â”œâ”€â”€ stacking_meta_model.pkl
â”‚ â””â”€â”€ best_threshold.txt
â”‚
â”œâ”€â”€ submissions/
â”‚ â””â”€â”€ stacking_submission.csv
â”‚
â”œâ”€â”€ mlruns/ # MLflow experiment tracking
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


## ğŸ”§ Tech Stack

- **Python**
- **Pandas, NumPy**
- **Scikit-learn**
- **XGBoost**
- **LightGBM**
- **SHAP (Explainable AI)**
- **MLflow (Experiment Tracking)**
- **Joblib (Model Serialization)**


## ğŸ”„ End-to-End Workflow

### **1ï¸âƒ£ Exploratory Data Analysis (EDA)**

Key insights:
- Strong class imbalance (fraud is very rare)
- Transaction time patterns were informative  
- Certain merchant and location features correlated with fraud  
- Missing values handled systematically  

ğŸ“ Notebook: `notebooks/01_eda.ipynb`

---

### **2ï¸âƒ£ Preprocessing & Feature Engineering (Industry-Grade)**

Key steps:
- Created **time-based features** (hour, day, weekday)
- Removed high-cardinality identifiers (`cc_num`, names, etc.)
- Imputed missing values appropriately  
- One-hot encoded categorical features  
- Standardized numeric features  
- Saved full pipeline as `models/preprocessor.pkl`

ğŸ“ Notebook: `notebooks/02_preprocessing.ipynb`  
ğŸ“ Script: `src/preprocessing.py`

---

### **3ï¸âƒ£ Modeling (Baseline â†’ Advanced)**

Models trained:
- Logistic Regression (baseline)
- Random Forest  
- Tuned XGBoost (**best single model**)  
- Tuned LightGBM  

ğŸ“ Notebooks:
- `03_baseline_models.ipynb`
- `04_xgboost_tuning.ipynb`
- `05_lightgbm_tuning.ipynb`

---

### **4ï¸âƒ£ Stacking Ensemble (Production-Level Approach)**

Base models:
- Logistic Regression  
- XGBoost  
- LightGBM  

Meta-model:
- Logistic Regression trained on predicted probabilities

ğŸ“ Notebook: `06_stacking_ensemble.ipynb`

---

### **5ï¸âƒ£ Threshold Tuning (Business-Ready)**

Instead of using a default 0.5 cutoff, I:
- Tuned decision threshold on validation set  
- Optimized for **F1-score** (better for fraud detection)
- Visualized F1 vs Threshold  
- Saved best threshold in `models/best_threshold.txt`

ğŸ“ Notebook: `09_threshold_tuning.ipynb`

---

### **6ï¸âƒ£ Model Explainability (SHAP)**

Used **SHAP TreeExplainer** on tuned XGBoost model to provide:
- Global feature importance  
- Local explanations for individual transactions  
- Summary plots for interpretability  

ğŸ“ Notebook: `10_shap_explainability.ipynb`



### **7ï¸âƒ£ Experiment Tracking (MLflow)**

Logged:
- Model parameters  
- Validation metrics (Accuracy, F1)  
- Trained model artifacts  

### **8ï¸âƒ£ Automated Inference & Submission
python -m src.make_submission
Generates:
submissions/stacking_submission.csv

ğŸš€ How to Run This Project
pip install -r requirements.txt

#Train models with MLflow
python -m src.train_with_mlflow

#Generate predictions
python -m src.make_submission



ğŸ¯ What I Learned 

Built a reproducible ML pipeline

Handled class imbalance effectively

Used feature engineering to boost performance

Applied stacking ensemble learning

Tuned decision threshold instead of default 0.5

Added explainability with SHAP

Tracked experiments using MLflow

Created production-style inference script

ğŸ“¬ Contact

GitHub: [abdulrab787](https://github.com/abdulrab787)

Kaggle:[abdurrabnizamuddeen](https://www.kaggle.com/abdurrabnizamuddeen)

LinkedIn:[abdulrab89](www.linkedin.com/in/abdulrab89)
